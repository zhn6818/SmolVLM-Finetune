#!/usr/bin/env python3
"""
å¿«é€ŸæŸ¥çœ‹SmolVLMæ¨¡å‹ç»“æ„çš„ç®€å•è„šæœ¬
"""

import torch
from transformers import AutoModelForVision2Seq, AutoProcessor

def quick_view_model():
    """å¿«é€ŸæŸ¥çœ‹æ¨¡å‹ç»“æ„"""
    print("æ­£åœ¨åŠ è½½SmolVLMæ¨¡å‹...")
    
    try:
        # åŠ è½½æ¨¡å‹
        model_id = "HuggingFaceTB/SmolVLM-Instruct"
        model = AutoModelForVision2Seq.from_pretrained(model_id, torch_dtype=torch.float32)
        
        print(f"\nâœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"æ¨¡å‹ç±»å‹: {type(model).__name__}")
        
        # æ‰“å°ä¸»è¦ç»„ä»¶
        print("\n" + "="*50)
        print("æ¨¡å‹ä¸»è¦ç»„ä»¶ç»“æ„")
        print("="*50)
        
        if hasattr(model, 'model'):
            print("ğŸ“ model (ä¸»æ¨¡å‹å®¹å™¨)")
            
            if hasattr(model.model, 'vision_model'):
                vision_model = model.model.vision_model
                vision_params = sum(p.numel() for p in vision_model.parameters())
                print(f"  ğŸ‘ï¸  vision_model (è§†è§‰æ¨¡å‹) - å‚æ•°: {vision_params:,}")
                
                # æ‰“å°è§†è§‰æ¨¡å‹çš„å­ç»„ä»¶
                for name, child in vision_model.named_children():
                    child_params = sum(p.numel() for p in child.parameters())
                    print(f"    â”œâ”€â”€ {name} - å‚æ•°: {child_params:,}")
            
            if hasattr(model.model, 'text_model'):
                text_model = model.model.text_model
                text_params = sum(p.numel() for p in text_model.parameters())
                print(f"  ğŸ’¬  text_model (è¯­è¨€æ¨¡å‹) - å‚æ•°: {text_params:,}")
                
                # æ‰“å°è¯­è¨€æ¨¡å‹çš„å­ç»„ä»¶
                for name, child in text_model.named_children():
                    child_params = sum(p.numel() for p in child.parameters())
                    print(f"    â”œâ”€â”€ {name} - å‚æ•°: {child_params:,}")
            
            if hasattr(model.model, 'connector'):
                connector = model.model.connector
                connector_params = sum(p.numel() for p in connector.parameters())
                print(f"  ğŸ”—  connector (è¿æ¥å™¨) - å‚æ•°: {connector_params:,}")
        
        if hasattr(model, 'lm_head'):
            lm_head = model.lm_head
            lm_params = sum(p.numel() for p in lm_head.parameters())
            print(f"  ğŸ¯  lm_head (è¯­è¨€æ¨¡å‹å¤´éƒ¨) - å‚æ•°: {lm_params:,}")
        
        # ç»Ÿè®¡æ€»å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"\nğŸ“Š æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
        # æ‰“å°æ¨¡å‹é…ç½®ä¿¡æ¯
        print(f"\nâš™ï¸  æ¨¡å‹é…ç½®ä¿¡æ¯:")
        print(f"  - æœ€å¤§åºåˆ—é•¿åº¦: {getattr(model.config, 'max_position_embeddings', 'N/A')}")
        print(f"  - éšè—å±‚ç»´åº¦: {getattr(model.config, 'hidden_size', 'N/A')}")
        print(f"  - æ³¨æ„åŠ›å¤´æ•°: {getattr(model.config, 'num_attention_heads', 'N/A')}")
        print(f"  - å±‚æ•°: {getattr(model.config, 'num_hidden_layers', 'N/A')}")
        
        print(f"\nğŸ‰ æ¨¡å‹ç»“æ„æŸ¥çœ‹å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ¨¡å‹IDæ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    quick_view_model() 