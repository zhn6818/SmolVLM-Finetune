#!/usr/bin/env python3
"""
SmolVLMçœŸå®å›¾åƒæµ‹è¯• - ä½¿ç”¨ç½‘ç»œå›¾ç‰‡æµ‹è¯•æ¨¡å‹æ€§èƒ½
"""

import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import requests
from io import BytesIO

def load_real_image():
    """åŠ è½½çœŸå®çš„ç½‘ç»œå›¾ç‰‡"""
    # ä½¿ç”¨ä¸€ä¸ªå…¬å¼€çš„ç¤ºä¾‹å›¾ç‰‡
    url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg"
    try:
        response = requests.get(url, timeout=10)
        image = Image.open(BytesIO(response.content)).convert("RGB")
        return image
    except:
        # å¦‚æœç½‘ç»œå›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„æœ¬åœ°å›¾ç‰‡
        image = Image.new('RGB', (500, 400), color='white')
        from PIL import ImageDraw, ImageFont
        draw = ImageDraw.Draw(image)
        
        # ç»˜åˆ¶ä¸€ä¸ªæˆ¿å­
        draw.rectangle([100, 200, 300, 350], fill='brown', outline='black', width=3)
        draw.polygon([(80, 200), (200, 120), (320, 200)], fill='red', outline='black', width=3)
        draw.rectangle([150, 250, 200, 300], fill='blue', outline='black', width=2)
        draw.rectangle([220, 250, 270, 300], fill='blue', outline='black', width=2)
        
        # ç»˜åˆ¶å¤ªé˜³
        draw.ellipse([350, 50, 400, 100], fill='yellow', outline='orange', width=2)
        
        # ç»˜åˆ¶äº‘æœµ
        draw.ellipse([50, 50, 120, 80], fill='white', outline='gray', width=1)
        draw.ellipse([80, 40, 150, 70], fill='white', outline='gray', width=1)
        
        return image

def test_smolvlm_real():
    """æµ‹è¯•SmolVLMåœ¨çœŸå®å›¾åƒä¸Šçš„æ€§èƒ½"""
    print("ğŸš€ SmolVLMçœŸå®å›¾åƒæµ‹è¯•")
    print("="*60)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model_id = "HuggingFaceTB/SmolVLM-Instruct"
        model = AutoModelForVision2Seq.from_pretrained(model_id, torch_dtype=torch.float16)
        processor = AutoProcessor.from_pretrained(model_id)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # åŠ è½½çœŸå®å›¾ç‰‡
        print("\nğŸ“¸ åŠ è½½çœŸå®å›¾ç‰‡...")
        image = load_real_image()
        
        print(f"å›¾ç‰‡å°ºå¯¸: {image.size}")
        
        # æµ‹è¯•1: å›¾åƒæè¿°
        print("\n" + "="*60)
        print("æµ‹è¯•1: å›¾åƒæè¿°")
        print("="*60)
        
        prompt1 = "<image>\nDescribe what you see in this image in detail."
        print(f"è¾“å…¥: {prompt1}")
        
        inputs1 = processor(text=prompt1, images=image, return_tensors="pt")
        
        with torch.no_grad():
            outputs1 = model.generate(
                **inputs1, 
                max_new_tokens=150, 
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        response1 = processor.decode(outputs1[0], skip_special_tokens=True)
        print(f"è¾“å‡º: {response1}")
        
        # æµ‹è¯•2: è§†è§‰é—®ç­”
        print("\n" + "="*60)
        print("æµ‹è¯•2: è§†è§‰é—®ç­”")
        print("="*60)
        
        prompt2 = "<image>\nWhat colors do you see in this image?"
        print(f"è¾“å…¥: {prompt2}")
        
        inputs2 = processor(text=prompt2, images=image, return_tensors="pt")
        
        with torch.no_grad():
            outputs2 = model.generate(
                **inputs2, 
                max_new_tokens=100, 
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        response2 = processor.decode(outputs2[0], skip_special_tokens=True)
        print(f"è¾“å‡º: {response2}")
        
        # æµ‹è¯•3: ç‰©ä½“è¯†åˆ«
        print("\n" + "="*60)
        print("æµ‹è¯•3: ç‰©ä½“è¯†åˆ«")
        print("="*60)
        
        prompt3 = "<image>\nWhat objects can you identify in this image?"
        print(f"è¾“å…¥: {prompt3}")
        
        inputs3 = processor(text=prompt3, images=image, return_tensors="pt")
        
        with torch.no_grad():
            outputs3 = model.generate(
                **inputs3, 
                max_new_tokens=100, 
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id
            )
        
        response3 = processor.decode(outputs3[0], skip_special_tokens=True)
        print(f"è¾“å‡º: {response3}")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_smolvlm_real()
