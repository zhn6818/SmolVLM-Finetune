#!/usr/bin/env python3
"""
SmolVLMçœŸå®å›¾åƒæµ‹è¯• - ä½¿ç”¨ç½‘ç»œå›¾ç‰‡æµ‹è¯•æ¨¡å‹æ€§èƒ½
"""

import torch
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import requests
from io import BytesIO
import re

def load_real_image():
    """åŠ è½½çœŸå®çš„ç½‘ç»œå›¾ç‰‡"""
    # ä½¿ç”¨ä¸€ä¸ªå…¬å¼€çš„ç¤ºä¾‹å›¾ç‰‡
    url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg"
    try:
        response = requests.get(url, timeout=10)
        image = Image.open(BytesIO(response.content)).convert("RGB")
        return image
    except:
        # å¦‚æœç½‘ç»œå›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„æœ¬åœ°å›¾ç‰‡
        image = Image.new('RGB', (500, 400), color='white')
        from PIL import ImageDraw, ImageFont
        draw = ImageDraw.Draw(image)
        
        # ç»˜åˆ¶ä¸€ä¸ªæˆ¿å­
        draw.rectangle([100, 200, 300, 350], fill='brown', outline='black', width=3)
        draw.polygon([(80, 200), (200, 120), (320, 200)], fill='red', outline='black', width=3)
        draw.rectangle([150, 250, 200, 300], fill='blue', outline='black', width=2)
        draw.rectangle([220, 250, 270, 300], fill='blue', outline='black', width=2)
        
        # ç»˜åˆ¶å¤ªé˜³
        draw.ellipse([350, 50, 400, 100], fill='yellow', outline='orange', width=2)
        
        # ç»˜åˆ¶äº‘æœµ
        draw.ellipse([50, 50, 120, 80], fill='white', outline='gray', width=1)
        draw.ellipse([80, 40, 150, 70], fill='white', outline='gray', width=1)
        
        return image

def smart_generate(model, processor, inputs, max_new_tokens=200, temperature=0.7, top_p=0.9):
    """
    æ™ºèƒ½ç”Ÿæˆå‡½æ•°ï¼Œå®ç°æ›´ä¼˜é›…çš„ç»ˆæ­¢
    """
    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_new_tokens=max_new_tokens, 
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=1.1,
            pad_token_id=processor.tokenizer.eos_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
            # ä¿®å¤warningï¼šnum_beams=1æ—¶ä¸ç”¨early_stopping
            early_stopping=False,
            num_beams=1,
            no_repeat_ngram_size=3,
            # è®¾ç½®æœ€å°é•¿åº¦ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹
            min_new_tokens=10,
            # ä½¿ç”¨æ›´æ™ºèƒ½çš„åœæ­¢ç­–ç•¥
            use_cache=True
        )
    
    # è§£ç è¾“å‡º
    response = processor.decode(outputs[0], skip_special_tokens=True)
    
    # åå¤„ç†ï¼šæ¸…ç†å’Œä¼˜åŒ–è¾“å‡º
    response = clean_response(response, inputs, processor)
    
    return response

def clean_response(response, inputs=None, processor=None):
    """
    æ¸…ç†å’Œä¼˜åŒ–æ¨¡å‹è¾“å‡ºï¼Œç§»é™¤é‡å¤çš„è¾“å…¥å†…å®¹
    """
    # è·å–åŸå§‹è¾“å…¥æ–‡æœ¬
    if inputs is not None and 'input_ids' in inputs and processor is not None:
        input_text = processor.decode(inputs['input_ids'][0], skip_special_tokens=True)
        # ç§»é™¤<image>æ ‡è®°åçš„å†…å®¹ä½œä¸ºè¾“å…¥éƒ¨åˆ†
        if "<image>" in input_text:
            input_parts = input_text.split("<image>")
            if len(input_parts) > 1:
                input_question = input_parts[-1].strip()
            else:
                input_question = input_text
        else:
            input_question = input_text
    else:
        input_question = ""
    
    # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if "<image>" in response:
        # æ‰¾åˆ°æœ€åä¸€ä¸ª<image>åçš„å†…å®¹
        parts = response.split("<image>")
        if len(parts) > 1:
            response = parts[-1].strip()
    
    # ç§»é™¤é‡å¤çš„è¾“å…¥é—®é¢˜
    if input_question and input_question in response:
        response = response.replace(input_question, "").strip()
    
    # ç§»é™¤é‡å¤çš„å¥å­
    sentences = response.split('.')
    unique_sentences = []
    seen = set()
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence not in seen and len(sentence) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
            unique_sentences.append(sentence)
            seen.add(sentence)
    
    # é‡æ–°ç»„åˆ
    response = '. '.join(unique_sentences)
    
    # ç¡®ä¿ä»¥å¥å·ç»“å°¾
    if response and not response.endswith(('.', '!', '?')):
        response += '.'
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    response = re.sub(r'\s+', ' ', response).strip()
    
    return response

def test_smolvlm_real():
    """æµ‹è¯•SmolVLMåœ¨çœŸå®å›¾åƒä¸Šçš„æ€§èƒ½"""
    print("ğŸš€ SmolVLMçœŸå®å›¾åƒæµ‹è¯•")
    print("="*60)
    
    try:
        # åŠ è½½æ¨¡å‹
        print("ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model_id = "HuggingFaceTB/SmolVLM-Instruct"
        model = AutoModelForVision2Seq.from_pretrained(model_id, torch_dtype=torch.float16)
        processor = AutoProcessor.from_pretrained(model_id)
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        
        # åŠ è½½çœŸå®å›¾ç‰‡
        print("\nğŸ“¸ åŠ è½½çœŸå®å›¾ç‰‡...")
        image = load_real_image()
        
        print(f"å›¾ç‰‡å°ºå¯¸: {image.size}")
        
        # æµ‹è¯•1: å›¾åƒæè¿°
        print("\n" + "="*60)
        print("æµ‹è¯•1: å›¾åƒæè¿°")
        print("="*60)
        
        prompt1 = "<image>\nDescribe what you see in this image in detail."
        print(f"è¾“å…¥: {prompt1}")
        
        inputs1 = processor(text=prompt1, images=image, return_tensors="pt")
        
        # ä½¿ç”¨æ™ºèƒ½ç”Ÿæˆå‡½æ•°
        response1 = smart_generate(model, processor, inputs1, max_new_tokens=200, temperature=0.7, top_p=0.9)
        print(f"è¾“å‡º: {response1}")
        
        # æµ‹è¯•2: è§†è§‰é—®ç­”
        print("\n" + "="*60)
        print("æµ‹è¯•2: è§†è§‰é—®ç­”")
        print("="*60)
        
        prompt2 = "<image>\nWhat colors do you see in this image?"
        print(f"è¾“å…¥: {prompt2}")
        
        inputs2 = processor(text=prompt2, images=image, return_tensors="pt")
        
        # ä½¿ç”¨æ™ºèƒ½ç”Ÿæˆå‡½æ•°
        response2 = smart_generate(model, processor, inputs2, max_new_tokens=150, temperature=0.7, top_p=0.9)
        print(f"è¾“å‡º: {response2}")
        
        # æµ‹è¯•3: ç‰©ä½“è¯†åˆ«
        print("\n" + "="*60)
        print("æµ‹è¯•3: ç‰©ä½“è¯†åˆ«")
        print("="*60)
        
        prompt3 = "<image>\nWhat objects can you identify in this image?"
        print(f"è¾“å…¥: {prompt3}")
        
        inputs3 = processor(text=prompt3, images=image, return_tensors="pt")
        
        # ä½¿ç”¨æ™ºèƒ½ç”Ÿæˆå‡½æ•°
        response3 = smart_generate(model, processor, inputs3, max_new_tokens=150, temperature=0.7, top_p=0.9)
        print(f"è¾“å‡º: {response3}")
        
        # æµ‹è¯•4: å¯¹æ¯”ä¸åŒç”Ÿæˆç­–ç•¥
        print("\n" + "="*60)
        print("æµ‹è¯•4: å¯¹æ¯”ä¸åŒç”Ÿæˆç­–ç•¥")
        print("="*60)
        
        prompt4 = "<image>\nWrite a short story about what you see in this image."
        print(f"è¾“å…¥: {prompt4}")
        
        inputs4 = processor(text=prompt4, images=image, return_tensors="pt")
        
        # ç­–ç•¥1: ä¿å®ˆç”Ÿæˆï¼ˆä½æ¸©åº¦ï¼‰
        print("\nğŸ“ ç­–ç•¥1: ä¿å®ˆç”Ÿæˆ (temperature=0.3)")
        response4a = smart_generate(model, processor, inputs4, max_new_tokens=200, temperature=0.3, top_p=0.8)
        print(f"è¾“å‡º: {response4a}")
        
        # ç­–ç•¥2: åˆ›æ„ç”Ÿæˆï¼ˆé«˜æ¸©åº¦ï¼‰
        print("\nğŸ¨ ç­–ç•¥2: åˆ›æ„ç”Ÿæˆ (temperature=0.9)")
        response4b = smart_generate(model, processor, inputs4, max_new_tokens=200, temperature=0.9, top_p=0.95)
        print(f"è¾“å‡º: {response4b}")
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_smolvlm_real()
